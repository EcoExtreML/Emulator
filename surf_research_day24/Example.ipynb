{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a72d8d0-e4fd-4b5e-ac80-992ae2e98b75",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cfe17-19dc-4c09-a33d-385637259cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8dec8-cc71-4376-b844-1e8de8e64fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2014-1-31\"\n",
    "end_time = \"2014-02-10\"\n",
    "year = 2014\n",
    "\n",
    "parent_in_path = f\"/gpfs/work2/0/ttse0619/qianqian/global_data_Qianqian/1input_data/{year}global\"\n",
    "data_paths = {\"era5land\": f\"{parent_in_path}/era5land/*.nc\",\n",
    "            \"lai\": f\"{parent_in_path}/lai_v2/*.nc\",\n",
    "            \"ssm\": f\"{parent_in_path}/ssm/GlobalGSSM11km2014_20240214.tif\",\n",
    "            \"co2\": f\"{parent_in_path}/co2/CAMS_CO2_2003-2020.nc\",\n",
    "            \"landcover\": f\"{parent_in_path}/igbp/landcover10km_global.nc\",\n",
    "            \"vcmax\": f\"{parent_in_path}/vcmax/TROPOMI_Vmax_Tg_mean10km_global.nc\",\n",
    "            \"canopyheight\": f\"{parent_in_path}/canopy_height/canopy_height_11kmEurope20230921_10km.nc\",\n",
    "            }\n",
    "\n",
    "parent_out_path = \"/scratch-shared/falidoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce58779-3d4b-49fa-a1c4-e0056b1397c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shape file\n",
    "eu_shape_file = \"/gpfs/work2/0/ttse0619/qianqian/global_data_Qianqian/1input_data/EuropeBoundary.shp\"\n",
    "gdf = gpd.read_file(eu_shape_file)\n",
    "bbox = gdf.total_bounds\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c63fbd-3b98-41d0-bda9-21ca5e1d0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era5_preprocess(ds):    \n",
    "    # Convert the longitude coordinates from [0, 360] to [-180, 180]\n",
    "    ds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180))\n",
    "    return ds\n",
    "\n",
    "def co2_preprocess(ds, start_time, end_time):    \n",
    "    ds = ds.sel(time=slice(start_time, end_time))\n",
    "    return ds\n",
    "\n",
    "co2_partial_func = partial(co2_preprocess, start_time=start_time, end_time=end_time)\n",
    "\n",
    "def fix_coords(ds):\n",
    "    if 'band' in ds.dims:\n",
    "        ds = ds.rename_dims({'band': 'time'})\n",
    "        ds = ds.rename_vars({'band': 'time'})\n",
    "\n",
    "    if 'x' in ds.dims and 'y' in ds.dims:\n",
    "        ds = ds.rename_dims({'x': 'longitude', 'y': 'latitude'})\n",
    "        ds = ds.rename_vars({'x': 'longitude', 'y': 'latitude'})\n",
    "        \n",
    "    elif 'lon' in ds.dims and 'lat' in ds.dims:\n",
    "        ds = ds.rename_dims({'lon': 'longitude', 'lat': 'latitude'})\n",
    "        ds = ds.rename_vars({'lon': 'longitude', 'lat': 'latitude'})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ee5b5-bb41-4ba0-8048-cf4fbe506ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=28, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb5e5d-f068-4bcf-9e03-8dfdbc5efaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 'auto'\n",
    "\n",
    "for data_path in data_paths:\n",
    "    \n",
    "    if data_path == \"era5land\":\n",
    "        ds = xr.open_mfdataset(data_paths[data_path], preprocess=era5_preprocess, chunks=chunks)\n",
    "    \n",
    "    if data_path == \"co2\":\n",
    "        ds = xr.open_mfdataset(data_paths[data_path], preprocess=co2_partial_func, chunks=chunks)\n",
    "        ds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180))       \n",
    "\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(data_paths[data_path], preprocess=fix_coords, chunks=chunks)\n",
    "        \n",
    "    # convert day of year\n",
    "    if ds.time.size == 1:\n",
    "        ds['time'] = [datetime.strptime(start_time, \"%Y-%m-%d\")]\n",
    "    elif ds.time.dtype == 'int64':\n",
    "        # Convert day of year to datetime\n",
    "        ds['time'] = [datetime(year, 1, 1) + timedelta(int(day) - 1) for day in ds.time.values]\n",
    "        \n",
    "    ds = ds.sortby(['longitude', 'latitude'])\n",
    "    masked_ds = ds.sel(longitude=slice(bbox[0], bbox[2]), latitude=slice(bbox[1], bbox[3]), time=slice(start_time, end_time))\n",
    "    masked_ds = masked_ds.chunk(chunks=chunks)\n",
    "    \n",
    "    # svae to zarr\n",
    "    out_path = f\"{parent_out_path}/{data_path}_{start_time}_{end_time}.zarr\"\n",
    "    masked_ds.to_zarr(out_path, mode='w')\n",
    "    print(f\"{out_path} is saved\")\n",
    "    print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7de4d-35d1-42d8-9947-21dd129b3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936245cf-a125-4602-bc78-6fd5ca917af4",
   "metadata": {},
   "source": [
    "## Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045e39b-1916-4e34-8bce-e08fdf0422d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5e7f0-773b-41d8-9f32-56d11859ab9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = \"2014-1-31\"\n",
    "end_time = \"2014-02-10\"\n",
    "year = 2014\n",
    "parent_in_path = \"/scratch-shared/falidoost\"\n",
    "data_paths = {\"era5land\": f\"{parent_in_path}/era5land_{start_time}_{end_time}.zarr\",\n",
    "              \"lai\": f\"{parent_in_path}/lai_{start_time}_{end_time}.zarr\",\n",
    "              \"ssm\": f\"{parent_in_path}/ssm_{start_time}_{end_time}.zarr\",\n",
    "              \"co2\": f\"{parent_in_path}/co2_{start_time}_{end_time}.zarr\",\n",
    "              \"landcover\": f\"{parent_in_path}/landcover_{start_time}_{end_time}.zarr\",\n",
    "              \"vcmax\": f\"{parent_in_path}/vcmax_{start_time}_{end_time}.zarr\",\n",
    "              \"canopyheight\": f\"{parent_in_path}/canopyheight_{start_time}_{end_time}.zarr\",\n",
    "            }\n",
    "parent_out_path = \"/scratch-shared/falidoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ea320-4944-482a-b7a2-bb223d1a2897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interpolation(ds, other):\n",
    "    # in time\n",
    "    ds_interpolated = ds.interp(coords={\"time\": other.time}, method='nearest', kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    \n",
    "    # in space\n",
    "    ds_interpolated = ds_interpolated.interp(coords={\"longitude\": other.longitude, \"latitude\": other.latitude}, method='linear')\n",
    "    \n",
    "    return ds_interpolated\n",
    "\n",
    "variable_names = {\"lai\": \"LAI\",\n",
    "                  \"ssm\": \"band_data\",\n",
    "                  \"co2\": \"co2\",\n",
    "                  \"canopyheight\": \"__xarray_dataarray_variable__\",\n",
    "                  \"vcmax\": \"__xarray_dataarray_variable__\",\n",
    "                  \"landcover\": \"lccs_class\"}  \n",
    "\n",
    "era5land = xr.open_zarr(data_paths[\"era5land\"])\n",
    "era5land = era5land.chunk(time=-1, latitude=100, longitude=100)\n",
    "\n",
    "all_data = era5land.copy()\n",
    "\n",
    "for name in variable_names:\n",
    "    ds = xr.open_zarr(data_paths[name])\n",
    "    ds = ds.chunk(time=-1, latitude=100, longitude=100)\n",
    "    ds_interpolated = interpolation(ds, era5land)    \n",
    "    all_data[name] = ds_interpolated[variable_names[name]]\n",
    "\n",
    "all_data = all_data.chunk(time=-1, latitude=100, longitude=100)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca23840-61ac-43e0-b7ae-d757ccf3525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=25, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8100b0-260d-44c4-8395-3aeed5863066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svae to zarr\n",
    "out_path = f\"{parent_out_path}/all_data_{start_time}_{end_time}.zarr\"\n",
    "encoding = {var: {'chunks': (all_data.sizes[\"time\"], 100, 100)} for var in all_data.data_vars}\n",
    "\n",
    "all_data.to_zarr(out_path, mode='w', encoding=encoding)\n",
    "print(f\"{out_path} is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac32989-4421-4dfa-8907-03c56d4d25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93078d-7ed8-4c23-8372-8958d4fb2483",
   "metadata": {},
   "source": [
    "## Variable derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06141206-18c5-4f1c-ae2c-935595b742d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from PyStemmusScope import variable_conversion as vc\n",
    "from dask_ml.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c425a8-2eff-4636-8a36-69f6a62696d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = \"2014-1-31\"\n",
    "end_time = \"2014-02-10\"\n",
    "\n",
    "parent_in_path = \"/scratch-shared/falidoost\"\n",
    "data_paths = {\"all_data\": f\"{parent_in_path}/all_data_{start_time}_{end_time}.zarr\",\n",
    "    \"igbp_table\": f\"{parent_in_path}/lccs_to_igbp_table.csv\",\n",
    "    \"igbp_class\": f\"{parent_in_path}/IGBP11unique.csv\",\n",
    "            }\n",
    "\n",
    "parent_out_path = \"/scratch-shared/falidoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b74e1-9e57-48fc-ab0a-500794f17afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# era5_land variables\n",
    "all_data = xr.open_zarr(data_paths[\"all_data\"])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57b78e-e0e7-4a19-b160-b4a76d97bf83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def era5land_accumulated_vars(ds, input_name, output_name, scale_factor):\n",
    "    input_da = ds[input_name] / scale_factor\n",
    "    output_da = input_da.diff(\"time\")\n",
    "    output_da[0::24] = input_da[1::24]  # accumulation starts at t01 instead of t00\n",
    "\n",
    "    t00 = xr.DataArray(np.nan, coords=input_da.isel(time=0).coords) # assign first t00 to none\n",
    "    output_da = xr.concat([output_da, t00], dim='time')\n",
    "    ds[output_name] = output_da\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbd8de-58e3-4163-bfab-931e5121195f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = era5land_accumulated_vars(all_data, \"ssrd\", \"Rin\", 3600)\n",
    "all_data = era5land_accumulated_vars(all_data, \"strd\", \"Rli\", 3600)\n",
    "all_data = era5land_accumulated_vars(all_data, \"tp\", \"Precip_msr\", 0.001) # to mm\n",
    "all_data[\"p\"] = all_data[\"sp\"] / 100  # Pa -> hPa\n",
    "all_data[\"Ta\"] = all_data[\"t2m\"] - 273.15  # K -> degC\n",
    "all_data[\"ea\"] = vc.calculate_es(all_data[\"d2m\"] - 273.15)*10 # *10 is for kPa -> hPa\n",
    "all_data[\"u\"] = (all_data[\"u10\"] ** 2 + all_data[\"v10\"] ** 2) ** 0.5\n",
    "all_data[\"ssm\"] = all_data[\"ssm\"] / 1000\n",
    "\n",
    "all_data = all_data.chunk(time=-1, latitude=100, longitude=100)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2046ee2-b2c4-4242-b9f8-dc264bde1655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to convert landcover to IGBP\n",
    "# lookup tables\n",
    "igbp_table = pd.read_csv(data_paths[\"igbp_table\"])\n",
    "\n",
    "igbp_class = pd.read_csv(data_paths[\"igbp_class\"])['0'].unique()\n",
    "\n",
    "# define one hot encoding for IGBP using dask-ml functions\n",
    "encoder = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    ")\n",
    "\n",
    "# Unsorted categories are not yet supported by dask-ml\n",
    "igbp_stemmus_scope = np.sort(igbp_table[\"IGBP_STEMMUS_SCOPE\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "encoder = encoder.fit(igbp_stemmus_scope)  \n",
    "        \n",
    "lookup_table = igbp_table.set_index(\"lccs_class\").T.to_dict('records')[0]\n",
    "\n",
    "def map_landcover_to_igbp(landcover_block):\n",
    "    # Create a new DataArray with \"no data\" to hold the mapped values \n",
    "    mapped_block = da.full_like(landcover_block, fill_value=\"No data\", dtype=\"U7\")\n",
    "\n",
    "    # For each key-value pair in the lookup table\n",
    "    for key, value in lookup_table.items():\n",
    "        # Where the landcover_block equals the current key, assign the corresponding value\n",
    "        mapped_block = da.where(landcover_block == key, value, mapped_block)\n",
    "    \n",
    "    return mapped_block\n",
    "        \n",
    "\n",
    "def landcover_to_igbp(ds, landcover_var_name, encoder):\n",
    "    landcover = ds[landcover_var_name]\n",
    "    \n",
    "    # Replace NaN values with \"No data\" or 255 in the table\n",
    "    landcover = da.where(da.isnan(landcover), 255, landcover)\n",
    "    \n",
    "    igbp = map_landcover_to_igbp(landcover)\n",
    "    igbp_reshaped = igbp.reshape(-1, 1)\n",
    "\n",
    "    transformed = encoder.transform(igbp_reshaped)\n",
    "    \n",
    "    # Select the columns that correspond to the categories in igbp_class\n",
    "    indices = [np.where(encoder.categories_[0] == category)[0][0] for category in igbp_class]    \n",
    "    transformed = transformed[:, indices]\n",
    "\n",
    "    # Replace zeros with np.nan\n",
    "    transformed = da.where(transformed == 0, np.nan, transformed)\n",
    "\n",
    "    # Add each column of the transformed array as a new variable in the dataset\n",
    "    for i in range(transformed.shape[1]):\n",
    "        ds[f\"IGBP_veg_long{i+1}\"] = ((\"time\", \"latitude\", \"longitude\"), transformed[:, i].reshape(igbp.shape))\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46d3e7-ac7d-4cf7-baef-f86e89fc0130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = landcover_to_igbp(all_data, \"landcover\", encoder)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b314e-bb0c-401e-9aa9-a3eabeb9c54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f2575-e735-4c4a-810d-f0b27674f9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svae to zarr\n",
    "out_path = f\"{parent_out_path}/model_input_{start_time}_{end_time}.zarr\"\n",
    "ds.to_zarr(out_path, mode='w')\n",
    "print(f\"{out_path} is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff536a9-aa38-4e79-8641-514fe2ebbef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa8223-1b6f-40cd-b916-e73706929378",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8b0c3-9fc7-4a32-9b5e-e5936d81382f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcb372-7fca-43b9-b22a-2ed3e96b353e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = \"2014-1-31\"\n",
    "end_time = \"2014-02-10\"\n",
    "\n",
    "parent_in_path = \"/scratch-shared/falidoost\"\n",
    "parent_out_path = \"/scratch-shared/falidoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce392f-fba4-42eb-8dd6-2a0376a2516a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "path_model = f\"{parent_in_path}/hourly_multi7_depth20_min1219.pkl\"\n",
    "with open(path_model, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dc494-b4b5-4c63-8cc3-75593c682312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename some variables\n",
    "model_input = xr.open_zarr(f\"{parent_in_path}/model_input_{start_time}_{end_time}.zarr\")\n",
    "rename_vars = {\"co2\": \"CO2\", \"lai\": \"LAI\", \"canopyheight\": \"hc\", \"ssm\": \"SSM\", \"vcmax\": \"Vcmo\"}\n",
    "ds = model_input.rename(rename_vars)\n",
    "\n",
    "input_vars = [\n",
    "    'Rin', 'Rli', 'p', 'Ta', 'ea', 'u', 'CO2', 'LAI', 'Vcmo','hc', 'Precip_msr',  \n",
    "    'SSM',  *[f'IGBP_veg_long{i}' for i in range(1, 12)]\n",
    "]\n",
    "\n",
    "# select input data \n",
    "input_ds = ds[input_vars]\n",
    "\n",
    "# define output template\n",
    "output_vars = ['LEtot','Htot','Rntot','Gtot', 'Actot','SIF685', 'SIF740']\n",
    "output_temp = xr.Dataset()\n",
    "ds_shape = (input_ds.sizes['time'], input_ds.sizes['latitude'], input_ds.sizes['longitude'])\n",
    "\n",
    "for var in output_vars:\n",
    "    output_temp[var] = xr.DataArray(\n",
    "        name = var,\n",
    "        data=da.zeros(ds_shape),\n",
    "        dims=input_ds.dims,\n",
    "        coords=input_ds.coords,\n",
    "    )\n",
    "output_temp = output_temp.chunk(input_ds.chunksizes) # the same cunkc as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4c78b-0e7a-4a4a-8000-a46b2a18b3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictFlux(input_ds, model, output_vars):\n",
    "\n",
    "    df_features = input_ds.to_dataframe().reset_index().drop(columns=[\"time\", \"longitude\", \"latitude\"])\n",
    "    \n",
    "    # Convert the nan value as 0 for the calculation\n",
    "    invalid_index = df_features.isnull().any(axis=1)\n",
    "    df_features.loc[invalid_index] = 0\n",
    "    \n",
    "    LEH = model.predict(df_features)\n",
    "    LEH[invalid_index] = np.nan # convert the original nan values to nan back\n",
    "    \n",
    "    output_ds = xr.Dataset(coords=input_ds.coords)\n",
    "    ds_shape = (output_ds.dims['time'], output_ds.dims['longitude'], output_ds.dims['latitude'])\n",
    "    \n",
    "    for i, name in enumerate(output_vars):\n",
    "        output_ds[name] = ((\"time\", \"longitude\", \"latitude\"), LEH[:, i].reshape(ds_shape))\n",
    "    \n",
    "    return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32086adc-07cf-4a87-a7fe-e87424ffcb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result\n",
    "LEH = xr.map_blocks(\n",
    "    predictFlux,\n",
    "    input_ds,\n",
    "    kwargs={\n",
    "        \"model\": model, \n",
    "        \"output_vars\": output_vars, \n",
    "    },\n",
    "    template=output_temp,\n",
    ")\n",
    "LEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31e13a-d959-4178-9425-3988eb8bcd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbef739-dbff-4b01-98fe-3ac2aa4d66e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svae to zarr\n",
    "out_path = f\"{parent_out_path}/predicted_{start_time}_{end_time}.zarr\"\n",
    "LEH.to_zarr(out_path, mode='w')\n",
    "print(f\"{out_path} is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3edbe-a7b0-4d6a-9afa-14564716af1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bf32d-efeb-4a6c-b81b-b812df28220c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
